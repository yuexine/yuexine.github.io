{"pages":[],"posts":[{"title":"Spring Cloud Eureka Client","text":"使用Eureka Client 在pom中包含jar 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 注册指定注册中心的位置 1234eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 注册认证HTTP Basic认证已经自动添加到eureka客户端中, 更复杂的需求，参考文档实现 1http://user:password@localhost:8761/eureka 源码分析在阅读源码的时候，我们根据它本身提供的功能去关注它实现的过程，以及代码是怎么被触发的 入口 通过观察spring-cloud-netflix-eureka-client包中的META-INF/spring.factories可以看到里面配置了一组EnableAutoConfiguration, 我们重点关注EurekaClientAutoConfiguration EurekaClientAutoConfiguration 在这个对象中，主要是根据条件创建了一组spring bean, 包括 EurekaClientConfigBean ManagementMetadataProvider EurekaInstanceConfigBean DiscoveryClient EurekaServiceRegistry EurekaClient EurekaClient 继续跟踪代码，观察EurekaClient的初始化过程 1234567891011121314 public class CloudEurekaClient extends DiscoveryClient { ... public CloudEurekaClient(ApplicationInfoManager applicationInfoManager, EurekaClientConfig config, AbstractDiscoveryClientOptionalArgs&lt;?&gt; args, ApplicationEventPublisher publisher) { super(applicationInfoManager, config, args); this.applicationInfoManager = applicationInfoManager; this.publisher = publisher; this.eurekaTransportField = ReflectionUtils.findField(DiscoveryClient.class, \"eurekaTransport\"); ReflectionUtils.makeAccessible(this.eurekaTransportField); } ...} 这里会调用父级的构造方法去执行注册、心跳、缓存刷新等任务 1234567891011121314151617181920212223242526272829 DiscoveryClient(ApplicationInfoManager applicationInfoManager, EurekaClientConfig config, AbstractDiscoveryClientOptionalArgs args, Provider&lt;BackupRegistry&gt; backupRegistryProvider) { ... // default size of 2 - 1 each for heartbeat and cacheRefresh scheduler = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-%d\") .setDaemon(true) .build()); heartbeatExecutor = new ThreadPoolExecutor( 1, clientConfig.getHeartbeatExecutorThreadPoolSize(), 0, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-HeartbeatExecutor-%d\") .setDaemon(true) .build() ); // use direct handoff cacheRefreshExecutor = new ThreadPoolExecutor( 1, clientConfig.getCacheRefreshExecutorThreadPoolSize(), 0, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-CacheRefreshExecutor-%d\") .setDaemon(true) .build() ); // use direct handoff ... } 到这里我们可以发现，spring cloud eureka client将这些任务都委托给了Netflix的Eureka组建去执行","link":"/2019/05/08/Spring-Cloud-Eureka-Client/"},{"title":"Jenkins集成Helm进行应用持续发布","text":"目标 在开发过程中，对开发的应用版本进行持续迭代并且发布到kubernetes集群中 使用Helm工具以chart升级的方式完成不同版本应用的发布升级 前提 在使用Helm之前，已经完成了Jenkins的部署，并且已经正常运行脚本与Kubernetes平台进行 通过kubectl命令执行配置文件完成应用的版本更新 实现步骤 helm chart制作，此步骤略 包含helm工具的jenkins容器镜像制作 制作完成的镜像文件 harbor.3incloud.com/library/jenkins-jnlp:latest 初始化应用安装 helm init --client-only &amp;&amp; helm repo add --username jenkins --password password myharbor https://harbor.3incloud.com/chartrepo/XXXX &amp;&amp; helm repo update 这里主要就是在slave中初始化helm，并且添加私有的chart仓库 Jenkins完成应用持续部署 helm upgrade --reuse-values --set-string image.tag='${build_tag}' deploy-name --version 0.1.0 myharbor/chart-name 通过重新设定chart的image.tag完成容器的版本升级，--reuse-values 表示在原本参数的基础上进行更新 完整的Pipeline脚本123456789101112131415161718192021222324252627node('worker') { stage('Clone') { echo \"1.Clone Stage\" git credentialsId: 'gitlab-auth', url: 'https://gitlab.3incloud.com/xxx/xxx-server.git' script { build_tag = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim() } } stage('Test') { echo \"2.Test Stage\" } stage('Build') { echo \"3.Build Docker Image Stage\" sh \"docker build -t harbor.3incloud.com/xxx/xxx-server:${build_tag} .\" } stage('Push') { echo \"4.Push Docker Image Stage\" withDockerRegistry(credentialsId: 'harbor-ci', url: 'https://harbor.3incloud.com') { sh \"docker push harbor.3incloud.com/xxx/xxx-server:${build_tag}\" } } stage('Deploy') { echo \"5. Deploy Stage\" sh \"helm init --client-only &amp;&amp; helm repo add --username jenkins --password password myharbor https://harbor.3incloud.com/chartrepo/xxx &amp;&amp; helm repo update\" sh \"helm upgrade --reuse-values --set-string image.tag='${build_tag}' avic-server --version 0.1.0 myharbor/xxx-server\" }}","link":"/2019/04/25/Jenkins集成Helm进行应用持续发布/"},{"title":"Spring Cloud Config Server","text":"功能 基于rest形式提供额外的配置（键值对或yaml文本） 配置值的加解密（对称或非对称） 与springboot集成简单 环境仓库你的配置数据应该存放在哪里？Spring Cloud Config提供了多种策略，比如可以是filesystem，git，vault等，这里我们使用git管理我们的配置数据 Config Server如何做到管理应用的多个环节配置呢，它主要是用了以下三个变量来对配置数据进行区分 {application} 对应到客户端的spring.application.name {profile} 对应到客户端spring.application.profile {label} 这是服务端标记版本化的配置文件集合 这样客户端spring boot应用在启动的时候通过指定应用启动的参数，从而获取对应的配置数据 一个客户端应用程序引导配置示例 12345spring: application: name: foo profiles: active: dev，mysql 使用git作为存储库的一些配置 123456789101112spring: cloud: config: server: git: uri: https：//example.com/my/{application} # 占位符 skipSslValidation: true # 跳过ssl证书验证，默认false timeout: 4 # 超时时间（秒） username: user password: pw # 配置认证 searchPaths: '{application}' # 带占位符的搜索路径 refreshRate: 0 # 刷新频率，0表示每次请求都会获取最新的配置 健康指标Config Server附带一个运行状况指示器，用于检查配置EnvironmentRepository是否正常，默认情况下指示器请求的{application}是app，{profile}是default 配置健康指示器 1234567891011spring: cloud: config: server: health: repositories: myservice: label: mylabel myservice-dev: name: myservice profiles: development 如果想要禁用健康检查需要设置 spring.cloud.config.server.health.enabled=false 另外在配置健康检查之后，如果需要获取详细的健康指标数据还需要做额外的配置 12345678management: endpoints: enabled-by-default: true web: base-path: /admin endpoint: health: show-details: always # 显示详细数据 安全通过集成spring-boot-starter-security使用默认的HTTP Basic来保护配置数据 配置用户名密码 12345spring: security: user: name: yuexin password: XXX 加密与解密不想看了，用的时候再去看文档 附上完整的配置 123456789101112131415161718192021222324252627282930spring: application: name: clivia-config-server cloud: config: server: git: uri: https://github.com/yuexine/cloud-repo.git username: yuexine password: XXX search-paths: clivia health: repositories: cloud-repo: name: foo profiles: dev security: user: name: yuexin password: XXXserver: port: 8888management: endpoints: enabled-by-default: true web: base-path: /admin endpoint: health: show-details: always 参考: 官方文档-基于版本2.1.0.RELEASE Spring cloud config Actuator健康监测","link":"/2019/05/06/Spring-Cloud-Config-Server/"},{"title":"Spring Cloud Eureka Server","text":"运行Eureka Server 将Eureka Server集成到应用中，只需要在pom中加入 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 接着在启动类上加入@EnableEurekaServer 最后配置application.yml 1234567891011server: port: 8761eureka: instance: hostname: localhost client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/ 高可用在生产环境下，往往会部署多个eureka实例，使它们相互注册，实现的方式是通过修改配置实现 12345678910111213141516171819---spring: profiles: peer1eureka: instance: hostname: peer1 client: serviceUrl: defaultZone: http://peer2/eureka/---spring: profiles: peer2eureka: instance: hostname: peer2 client: serviceUrl: defaultZone: http://peer1/eureka/ 这样客户端在使用的时候，在指定注册中心时，指定多个，用逗号隔开 1234eureka: client: serviceUrl: defaultZone: http://peer1/eureka/,http://peer2/eureka 安全的 Eureka Server通过加入spring-boot-starter-security，使服务的注册与发现操作开启认证。 禁用CSRF 123456789@EnableWebSecurityclass WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http.csrf().ignoringAntMatchers(\"/eureka/**\"); super.configure(http); }} 在配置中加入 12345spring: security: user: name: admin password: pwd 修改defaultZone，指定name&amp;password，这里需要注意的是，所有的配置defaultZone的地方都需要配置name&amp;password，包括Eureka Server本身 123456789eureka: instance: hostname: localhost prefer-ip-address: true # 这里使用ip而不是默认的hostname client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://admin:pwd@${eureka.instance.hostname}:${server.port}/eureka/","link":"/2019/05/09/Spring-Cloud-Eureka-Server/"},{"title":"Spring WebFlux源码学习笔记(二)","text":"继续前面的学习 先看看WebHandler从前面的过程中我们知道，HttpHandler将Request请求最后交给了WebHandler去处理，我们先简单学习一下WebHandler的实现类 WebHandler实现类在WebHandler的实现类里面包含了一个装饰器对象WebHandlerDecorator，HttpWebHandlerAdapter也是继承自它 ExceptionHandlingWebHandlerWebHandler装饰器调用一个或多个WebExceptionHandlers处理webHandle过程中的错误 1234567891011121314151617@Overridepublic Mono&lt;Void&gt; handle(ServerWebExchange exchange) { Mono&lt;Void&gt; completion; try { completion = super.handle(exchange); //1 } catch (Throwable ex) { completion = Mono.error(ex); } for (WebExceptionHandler handler : this.exceptionHandlers) { completion = completion.onErrorResume(ex -&gt; handler.handle(exchange, ex)); //2 } return completion;} 调用父级的去处理请求 发生异常后进行异常解析 FilteringWebHandlerFilteringWebHandler使用一个WebFilterChain去处理ServerWebExchange 1private final DefaultWebFilterChain chain; 123456@Overridepublic Mono&lt;Void&gt; filter(ServerWebExchange exchange) { return Mono.defer(() -&gt; this.currentFilter != null &amp;&amp; this.next != null ? this.currentFilter.filter(exchange, this.next) : this.handler.handle(exchange)); 这里的handle实际上是DispatcherHandler DispatcherHandlerDispatcherHandler应该是WebFlux核心的对象，在这里完成了对请求的处理。 123456789101112@Overridepublic Mono&lt;Void&gt; handle(ServerWebExchange exchange) { if (this.handlerMappings == null) { return createNotFoundError(); } return Flux.fromIterable(this.handlerMappings) //1 .concatMap(mapping -&gt; mapping.getHandler(exchange)) //2 .next() //3 .switchIfEmpty(createNotFoundError()) //4 .flatMap(handler -&gt; invokeHandler(exchange, handler)) //5 .flatMap(result -&gt; handleResult(exchange, result)); //6} 将handlerMappings打包成一个广播对象 使用concatMap操作符收集mapping.getHandler(exchange)返回的对象。具体Handle过程我们后面再看。 next()取出第一个对象 如果没有找到映射函数就广播一个异常Mono 调用业务代码函数处理请求 对业务代码结果进行处理 从WebHandlerDecorator开始SpringWebFlux是怎么将这个过程串起来的呢？ 首先，HttpWebHandlerAdapter将请求委派给 ExceptionHandlingWebHandler，ExceptionHandlingWebHandler收集并处理过程中的异常，它使用FilteringWebHandler来处理ServerWebExchange。 FilteringWebHandler本身也不处理ServerWebExchange，继续将任务交给了DispatcherHandler去执行。","link":"/2019/05/22/Spring-WebFlux源码学习笔记(二)/"},{"title":"Vim资料整理","text":"vim命令速查 MacVim下载 Vim文档中文翻译","link":"/2019/05/24/Vim资料整理/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/04/24/hello-world/"},{"title":"Spring Cloud OpenFeign","text":"理解OpenFeign它声明一个REST客户端，来完成远程服务的调用，Feign会动态实现由JAX-RS或Spring MVC注解修饰的接口，主要用途是用来简化服务之间的相互调用 在SpringBoot中集成Feign前提已经准备好由SpringBoot构建的客户端和服务端，并且已经成功注册到Eureka注册中心。 引入相关jar在客户端中Pom中加入 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-okhttp&lt;/artifactId&gt;&lt;/dependency&gt; 相关配置在客户端启动类上加上注解@EnableFeignClients 使用声明一个接口，在接口的方法上引入SpringMvc的注解，即可完成远程服务的调用 123456@FeignClient(name = \"clivia-service-notification-${spring.profiles.active}\")public interface NotificationClient { @RequestMapping(method = RequestMethod.GET, value = \"/\") String home();} 使用Feign使用OKHTTP前面在引入jar包的时候，已经集成了okhttp相关的包 在理解这一块内容的时候，简单看了一下这一块的实现，根据官方的文档介绍 Spring Cloud Netflix提供了一些Bean，包括了Feign Client，在Ribbon开启的情况下，默认的FeignClient是LoadBalancerFeignClient 跟踪LoadBalancerFeignClient代码，它在构造器上定义了一个委派的Client,继续看这个对象。他的实现在接口内，并且最终使用的是HttpURLConnection, 也就是Java原生的远程连接对象。 回到主题，在这个包中，有一个配置类对象OkHttpFeignLoadBalancedConfiguration，开启条件是feign.okhttp.enabled=true，并且OkHttpClient存在于classpath中 继续观察当前对象 123456789101112131415161718192021222324252627282930313233343536373839404142@Configuration@ConditionalOnMissingBean(okhttp3.OkHttpClient.class)protected static class OkHttpFeignConfiguration { private okhttp3.OkHttpClient okHttpClient; &lt;!--定义了一个连接池--&gt; @Bean @ConditionalOnMissingBean(ConnectionPool.class) public ConnectionPool httpClientConnectionPool( FeignHttpClientProperties httpClientProperties, OkHttpClientConnectionPoolFactory connectionPoolFactory) { Integer maxTotalConnections = httpClientProperties.getMaxConnections(); Long timeToLive = httpClientProperties.getTimeToLive(); TimeUnit ttlUnit = httpClientProperties.getTimeToLiveUnit(); return connectionPoolFactory.create(maxTotalConnections, timeToLive, ttlUnit); } &lt;!--创建了一个OkHttpClient客户端--&gt; @Bean public okhttp3.OkHttpClient client(OkHttpClientFactory httpClientFactory, ConnectionPool connectionPool, FeignHttpClientProperties httpClientProperties) { Boolean followRedirects = httpClientProperties.isFollowRedirects(); Integer connectTimeout = httpClientProperties.getConnectionTimeout(); this.okHttpClient = httpClientFactory .createBuilder(httpClientProperties.isDisableSslValidation()) .connectTimeout(connectTimeout, TimeUnit.MILLISECONDS) .followRedirects(followRedirects).connectionPool(connectionPool) .build(); return this.okHttpClient; } @PreDestroy public void destroy() { if (this.okHttpClient != null) { this.okHttpClient.dispatcher().executorService().shutdown(); this.okHttpClient.connectionPool().evictAll(); } } } 通过对以上代码的理解，我只需要在配置中设置feign.okhttp.enabled=true，即可使用OkHttp作为服务调用的客户端工具，当然，如果在代码中自己配置了OkHttpClientBean，框架本身就不会再重复创建了，相关连接池的配置读取自配置feign.httpclient.* 最后还需要注意的就是设置Feign客户端远程调用的连接和读取的超时时间配置，这个配置需要在客户端中额外配置，默认的配置示例 1234567feign: client: config: default: # 客户端名称，default表示默认 connectTimeout: 6000 readTimeout: 6000 # 表示6s loggerLevel: basic 通过修改OKHttp连接池和请求相应相关的参数来获取更好的性能 日志配置Feign日志记录仅响应DEBUG级别日志，所以首先配置Feign Client的日志级别为DEBUG 123logging: level: com.vcors.project.common.feign.NotificationClient: DEBUG 接下来配置Logger.Level，这里可以单独为每个客户端配置，也可以统一配置，下面的示例是配置到一个class中 123456789@Configurationpublic class FooConfiguration { @Bean Logger.Level feignLoggerLevel() { return Logger.Level.FULL; }}@FeignClient(name = \"clivia-service-notification-${spring.profiles.active}\", configuration = {DefaultFeignConfiguration.class}) 日志级别 None 无日志，默认 BASIC 记录请求URL和方法及响应时间和状态 HEADERS 记录基本信息及请求响应头 FULL 记录全部的信息 数据压缩通过GZIP压缩请求 1234567feign: compression: request: enabled: true min-request-size: 2048 response: enabled: true HTTP BASIC认证对服务的API增加访问保护，是非常有意义的，被请求的服务通过集成spring-boot-starter-security, 可以快速使用HTTP BASIC 这个时候Feign在调用远程服务的时候，通过加入拦截器，可以完成HTTP BASIC的验证 12345678910111213141516171819@Configurationpublic class DefaultFeignConfiguration { @Bean Logger.Level feignLoggerLevel() { return Logger.Level.BASIC; } &lt;!--请求重试--&gt; @Bean Retryer feignRetryer() { return Retryer.NEVER_RETRY; } @Bean public BasicAuthRequestInterceptor basicAuthRequestInterceptor() { return new BasicAuthRequestInterceptor(\"admin\", \"passwd\"); }}","link":"/2019/05/09/Spring-Cloud-OpenFeign/"},{"title":"Spring WebFlux源码学习笔记(一)","text":"在开始学习WebFlux之前，需要对Java的函数式编程有一定的了解，熟悉相关函数式接口的用法，包括Function，BiFunction等 开始学习开始，我们需要找一个切入点，了解一次Http请求发生后，代码响应的过程，这里我们从reactor.netty.http.server的HttpServerHandle开始看起 1234567891011121314151617public void onStateChange(Connection connection, State newState) { if (newState == HttpServerState.REQUEST_RECEIVED) { //1 try { if (log.isDebugEnabled()) { log.debug(format(connection.channel(), \"Handler is being applied: {}\"), handler); } HttpServerOperations ops = (HttpServerOperations) connection; //2 Mono.fromDirect(handler.apply(ops, ops)) .subscribe(ops.disposeSubscriber()); //3 } catch (Throwable t) { log.error(format(connection.channel(), \"\"), t); connection.channel() .close(); } }} 接受一个State为REQUEST_RECEIVED状态的连接 获取在前面的过程中已经将连接信息封装到了HttpServerOperations对象中，这个对象实现了HttpServerRequest, HttpServerResponse 12 class HttpServerOperations extends HttpOperations&lt;HttpServerRequest, HttpServerResponse&gt;implements HttpServerRequest, HttpServerResponse 创建了一个Mono对象(Reactor中的用来发出一个元素)，并且订阅(触发)了这个Publisher。 备注: Mono.fromDirect() 用来转换一个Publisher到Mono免去了一些基础的验证。 处理适配器:ReactorHttpHandlerAdapter从开始的过程第3步中使用了ReactorHttpHandlerAdapter对HttpServerRequest, HttpServerResponse进行了处理 1public class ReactorHttpHandlerAdapter implements BiFunction&lt;HttpServerRequest, HttpServerResponse, Mono&lt;Void&gt;&gt; ReactorHttpHandlerAdapter实现了BiFunction，函数方法接收2个参数，分别是HttpServerRequest, HttpServerResponse，返回Mono，看类名可以看出这是一个适配器对象，实际的Handler为 12345678910111213141516171819202122public Mono&lt;Void&gt; apply(HttpServerRequest reactorRequest, HttpServerResponse reactorResponse) { NettyDataBufferFactory bufferFactory = new NettyDataBufferFactory(reactorResponse.alloc()); try { ReactorServerHttpRequest request = new ReactorServerHttpRequest(reactorRequest, bufferFactory); //1 ServerHttpResponse response = new ReactorServerHttpResponse(reactorResponse, bufferFactory); //2 if (request.getMethod() == HttpMethod.HEAD) { response = new HttpHeadResponseDecorator(response); } return this.httpHandler.handle(request, response) //3 .doOnError(ex -&gt; logger.trace(request.getLogPrefix() + \"Failed to complete: \" + ex.getMessage())) //4 .doOnSuccess(aVoid -&gt; logger.trace(request.getLogPrefix() + \"Handling completed\")); //5 } catch (URISyntaxException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Failed to get request URI: \" + ex.getMessage()); } reactorResponse.status(HttpResponseStatus.BAD_REQUEST); return Mono.empty(); }} 包装HttpServerRequest到ReactorServerHttpRequest 包装HttpServerResponse到ReactorServerHttpResponse 调用ReactiveWebServerApplicationContext的内部类ServerManager完成handle。 出错时打印错误日志 成功是打印成功日志 注： doOnError()和doOnSuccess()不会对序列造成改变 继续任务委派在ServerManager里面包含了HttpHandler对象，实际的实现对象为HttpWebHandlerAdapter 12345678910111213141516@Overridepublic Mono&lt;Void&gt; handle(ServerHttpRequest request, ServerHttpResponse response) { if (this.forwardedHeaderTransformer != null) { request = this.forwardedHeaderTransformer.apply(request); } ServerWebExchange exchange = createExchange(request, response); //1 LogFormatUtils.traceDebug(logger, traceOn -&gt; exchange.getLogPrefix() + formatRequest(exchange.getRequest()) + (traceOn ? \", headers=\" + formatHeaders(exchange.getRequest().getHeaders()) : \"\")); return getDelegate().handle(exchange) //2 .doOnSuccess(aVoid -&gt; logResponse(exchange)) //3 .onErrorResume(ex -&gt; handleUnresolvedError(exchange, ex)) //4 .then(Mono.defer(response::setComplete)); //5} 将request和response组合成了一个对象ServerWebExchange 调用被委派对象处理这个组合后的对象 执行成功后打印response日志 捕获handle过程中的异常，处理后返回一个Mono对象 用 Mono 来表示序列已经结束 1234@Overridepublic Mono&lt;Void&gt; setComplete() { return !isCommitted() ? doCommit(null) : Mono.empty(); //6} 12345678910111213141516171819202122protected Mono&lt;Void&gt; doCommit(@Nullable Supplier&lt;? extends Mono&lt;Void&gt;&gt; writeAction) { if (!this.state.compareAndSet(State.NEW, State.COMMITTING)) { return Mono.empty(); } this.commitActions.add(() -&gt; Mono.fromRunnable(() -&gt; { applyStatusCode(); applyHeaders(); applyCookies(); this.state.set(State.COMMITTED); })); //7 if (writeAction != null) { this.commitActions.add(writeAction); } List&lt;? extends Mono&lt;Void&gt;&gt; actions = this.commitActions.stream() .map(Supplier::get).collect(Collectors.toList()); //8 return Flux.concat(actions).then(); //9} 设置处理完成，提交了一个空的writeAction 添加响应状态码，响应头，cookie等 转换为一个类型Mono的数组 将3步骤产生的数组用打包成一个Mono返回 小结从过程上看，当收到一个Http请求后，首先交给HttpHandlerAdapter类去处理，经过对数据的一些组合和处理，然后将请求丢给HttpWebHandlerAdapter去处理，从设计上看，WebHandler只是HttpHandler的一个子集，HttpWebHandlerAdapter本身是一个适配器对象，它会将任务继续委派给FilteringWebHandler","link":"/2019/05/21/Spring-WebFlux源码学习笔记(一)/"},{"title":"TCP与UDP","text":"传输层的作用作为传输层的两个主要协议，为了识别自己传输的数据属于哪个应用，在协议内设定了端口号 将接收和发送的数据与特定的应用程序进行关联起来,通过socket，也就是绑定了端口 区别 TCP 是面向连接的，可靠的的流协议，它提供了可靠性传输，实行顺序控制`重发控制流量控制拥塞控制提高网络利用率`等多种功能 UDP 是不具有可靠性的数据报协议，细微的处理交给上层的应用去完成 TCP头部就不解释每个字段的意思了 TCP的特点通过序列号和确认应答提高可靠性 数据包丢失 数据无法到达 数据应答丢失 重发的超时时间如何确定在BSD的Unix以及Windows系统中，超时都是以0.5s为单位进行控制的，因此重发超时的都是0.5的整数倍，不过由于最初的数据包还不知道往返时间，所以其重发的超时一般设置在6s左右。 连接管理这个会经常提到 TCP以段发送数据在建立TCP连接的同时，也可以确定发送数据包的单位，称为最大消息长度（MSS），TCP在传送大量数据时，是以MSS的大小将数据进行分割发送。 利用窗口控制提高速度TCP以1个段为单位，每发一个段进行一次确认应答，性能太差了，为了解决这个问题。 确认应答不再是以每个分段，而是以更大的单位进行确认，转发的时间被大幅度缩短。也就是说，发送端主机，在发送了一个段以后不必要一直等待确认应答，而是继续发送 窗口控制与重发控制 流控制为了防止流量浪费，比如说，数据接收端在高负荷的情况下无法接受任何数据。如果将本来接受的数据丢弃，则会触发重发机制。 为了防止这种现象，TCP提供了一种机制可以让发送端根据接收端的实际接受能力控制发送的数据量。这就是流控制。 做法就是在TCP首部中，有一个字段来通知窗口的大小，值越大，吞吐量越高 拥塞控制慢启动，慢慢变快 捎带应答 UDP头部","link":"/2019/05/08/TCP与UDP/"},{"title":"gRPC 入门学习","text":"功能gRPC是Google公司开源的一个远程服务调用框架，基于HTTP2协议。 特点 通过接口定义语言(IDL)定义了客户端与服务端数据交换的格式，该文件的扩展名为.proto, 通过将文件编译到对应的开发语言来完成系统的集成。 gRPC使用高效的序列化工具Protocol buffer 在SpringBoot中使用gRPC完成服务间调用完整的代码地址 方法及参数对象声明首先我们需要声明服务调用的接口方法与参数，推荐使用proto3，通过在pom中使用protobuf-maven-plugin插件，可以在编译的时候将.proto文件生成java class文件。 1234567891011121314151617syntax = \"proto3\";option java_multiple_files = true;package com.vcors.demo.grpcproto;message Person { string first_name = 1; string last_name = 2;}message Greeting { string message = 1;}service HelloWorldService { rpc sayHello (Person) returns (Greeting);} 服务端开发理解.proto代码生成 对于.proto文件中定义的对象，会生成对应的JavaClass，这是可以通过Protocal Buffers序列化的对象模型 对于.proto文件中定义的方法，会生成XXXGrpc XXXGrpc 接口方法声明 1public static abstract class HelloWorldServiceImplBase implements io.grpc.BindableService {} 这个抽象内部类主要用来集成在服务端，作为方法实现的父级接口 方法存根 种类分为 XXXBlockingStub blocking-style stub that supports unary and streaming output calls on the service XXXFutureStub ListenableFuture-style stub that supports unary calls on the service XXXStub async stub that supports all call types for the service 方法存根用来集成到客户端，方法客户端调用在服务端实现的方法。 集成依赖在服务端中集成proto模块声明的模型和接口方法 12345&lt;dependency&gt; &lt;groupId&gt;com.vcors.demo&lt;/groupId&gt; &lt;artifactId&gt;grpc-proto&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 方法实现通过继承HelloWorldServiceGrpc.HelloWorldServiceImplBase来重写.proto接口中声明的方法，也就是具体业务需要实现的逻辑代码都在这 12345678910111213141516171819@GRpcServicepublic class HelloWorldServiceImpl extends HelloWorldServiceGrpc.HelloWorldServiceImplBase { private static final Logger LOGGER = LoggerFactory.getLogger(HelloWorldServiceImpl.class); @Override public void sayHello(Person request, StreamObserver&lt;Greeting&gt; responseObserver) { LOGGER.info(\"server received {}\", request); String message = \"Hello \" + request.getFirstName() + \" \" + request.getLastName() + \"!\"; Greeting greeting = Greeting.newBuilder().setMessage(message).build(); LOGGER.info(\"server responded {}\", greeting); responseObserver.onNext(greeting); responseObserver.onCompleted(); } 客户端调用服务端创建存根接口首先在客户端中也同样要在pom中集成proto声明模块 创建存根方法对象 1234ManagedChannel managedChannel = ManagedChannelBuilder .forAddress(\"localhost\", 6565).usePlaintext().build(); helloWorldServiceBlockingStub = HelloWorldServiceGrpc.newBlockingStub(managedChannel); 执行方法调用像调用本地方法一样，实现不同应用接口的调用 12Greeting greeting = helloWorldServiceBlockingStub.sayHello(person); 测试同时启动服务端与客户端应用，调用客户端接口来收到服务端的正确响应 理解gRPC-spring-boot-startergRPC-spring-boot-starter 是对GRpcServer的一个封装，可以使我们在spring-boot项目中更简单的使用GRpc，核心功能就是启动了GRpcServer，并将通过@GRpcServer注解的Service注册到Server中 理解GRpcAutoConfiguration在grpc-spring-boot-starter/META-INF/spring.factories配置了GRpcAutoConfiguration, springboot启动时会自动加载该配置文件 该文件主要初始化了以下几个Bean实例 GRpcServerRunner HealthStatusManager GRpcServerBuilderConfigurer 理解GRpcServerRunner启动一个GRpc Server, 集成到SpringBoot中后，项目无需再集成其他的web server 123456789101112131415161718192021222324252627public void run(String... args) throws Exception { log.info(\"Starting gRPC Server ...\"); Collection&lt;ServerInterceptor&gt; globalInterceptors = (Collection)this.getBeanNamesByTypeWithAnnotation(GRpcGlobalInterceptor.class, ServerInterceptor.class).map((name) -&gt; { return (ServerInterceptor)this.applicationContext.getBeanFactory().getBean(name, ServerInterceptor.class); }).collect(Collectors.toList()); this.serverBuilder.addService(this.healthStatusManager.getHealthService()); this.getBeanNamesByTypeWithAnnotation(GRpcService.class, BindableService.class).forEach((name) -&gt; { BindableService srv = (BindableService)this.applicationContext.getBeanFactory().getBean(name, BindableService.class); ServerServiceDefinition serviceDefinition = srv.bindService(); GRpcService gRpcServiceAnn = (GRpcService)this.applicationContext.findAnnotationOnBean(name, GRpcService.class); serviceDefinition = this.bindInterceptors(serviceDefinition, gRpcServiceAnn, globalInterceptors); this.serverBuilder.addService(serviceDefinition); String serviceName = serviceDefinition.getServiceDescriptor().getName(); this.healthStatusManager.setStatus(serviceName, ServingStatus.SERVING); log.info(\"'{}' service has been registered.\", srv.getClass().getName()); }); if (this.gRpcServerProperties.isEnableReflection()) { this.serverBuilder.addService(ProtoReflectionService.newInstance()); log.info(\"'{}' service has been registered.\", ProtoReflectionService.class.getName()); } this.configurer.configure(this.serverBuilder); this.server = this.serverBuilder.build().start(); this.applicationContext.publishEvent(new GRpcServerInitializedEvent(this.server)); log.info(\"gRPC Server started, listening on port {}.\", this.server.getPort()); this.startDaemonAwaitThread();} 理解@GRpcService通过该注解声明的方法会被声明为Spring Bean，本质上它是一个@Service注解，另外就是在启动GRpcServer时会注册该Bean。 Next 接口调用的认证处理 结合Kubernetes平台实现服务发现与负载均衡","link":"/2019/05/17/gRPC-入门学习/"},{"title":"理解Function和BiFunction","text":"Function Function作为一个函数式接口，主要方法apply接受一个参数，返回一个值 1234567891011@FunctionalInterfacepublic interface Function&lt;T, R&gt; { /** * Applies this function to the given argument. * * @param t the function argument * @return the function result */ R apply(T t);} 这里需要注意的是定义的范型，这个接口中声明了2个类型，其中T为方法参数类型，R为方法返回类型 同时该接口包含了2个default方法，用来对函数进行组合 1234567891011&lt;!--这个方法用来组合一个函数，被组合的函数先执行，--&gt;default &lt;V&gt; Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) { Objects.requireNonNull(before); return (V v) -&gt; apply(before.apply(v));}&lt;!--同样这也用来组合一个函数，区别在于被组合的函数后执行--&gt;default &lt;V&gt; Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) { Objects.requireNonNull(after); return (T t) -&gt; after.apply(apply(t));} 使用 上面的例子中声明了2个函数f1和f2，分别使用不同的组合方法。 12345678910@Testpublic void test() { Function&lt;Integer, Integer&gt; f1 = x -&gt; {return x * x;}; Function&lt;Integer, Integer&gt; f2 = x -&gt; {return x + x;}; //（3+3）*（3+3）= 36 System.out.println(f1.compose(f2).apply(3)); //（3*3）+（3*3）=18 System.out.println(f1.andThen(f2).apply(3));} BiFunction BiFunction也是一个函数式接口，和Function接口不同的是，它在接口中声明了3个泛型，其中前两个作为方法参数类型，最后一个作为返回类型 123456789101112@FunctionalInterfacepublic interface BiFunction&lt;T, U, R&gt; { /** * Applies this function to the given arguments. * * @param t the first function argument * @param u the second function argument * @return the function result */ R apply(T t, U u);} 同时在BiFunction接口中定义了一个default方法andThen,用来与Function函数进行组合 使用 定义一个函数实现，然后对BiFunction和Function进行组合 12345678@Testpublic void test() { BiFunction&lt;String, String, String&gt; f1 = (x,y) -&gt; \"world \" + x + y; Function&lt;String, String&gt; f2 = (x) -&gt; \"hello \" + x; System.out.println(f1.apply(\"zhang\", \"san\")); System.out.println(f1.andThen(f2).apply(\"li\", \"si\"));} 其它函数式接口在理解了Function接口之后再去理解这些函数接口道理都是一样的，主要区别在于接口声明中范型的的用法 Supplier这个接口用来生产一个T类型的对象 12345678910@FunctionalInterfacepublic interface Supplier&lt;T&gt; { /** * Gets a result. * * @return a result */ T get();} Consumer这个接口用来消费一个对象，无返回 12345678910@FunctionalInterfacepublic interface Consumer&lt;T&gt; { /** * Performs this operation on the given argument. * * @param t the input argument */ void accept(T t);} Predicate这个接口用来测试一个对象,返回值为一个boolean类型 123456789101112@FunctionalInterfacepublic interface Predicate&lt;T&gt; { /** * Evaluates this predicate on the given argument. * * @param t the input argument * @return {@code true} if the input argument matches the predicate, * otherwise {@code false} */ boolean test(T t);}","link":"/2019/05/20/理解Function和BiFunction/"},{"title":"理解CompletableFuture","text":"CompletableFuture是java8新增的一个接口，它提供了对异步程序执行的另一种方式。 理解Future在结构上看CompletableFuture实现了Future和CompletionStage这两个接口，先来看看Future接口提供的功能 12345678public interface Future&lt;V&gt; { boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;} Future是一个将来对象，用来接收异步操作执行后的结果，提供了三个方法用来获取当前异步线程执行的状态。而调用get()方法会阻塞异步线程，直到执行完成获取结果。 但在实际异步编程的时候，使用起来并不是那么特别好用。 一个简单的例子： 123456789101112131415161718192021@Testpublic void test5() throws ExecutionException, InterruptedException { ExecutorService executorService = Executors.newFixedThreadPool(3); Future&lt;String&gt; f = executorService.submit(()-&gt;{ System.out.println(Thread.currentThread().getName()); try { Thread.sleep(3000L); } catch (InterruptedException e) { e.printStackTrace(); } return \"async finished\"; }); try { Thread.sleep(5000L); } catch (InterruptedException e) { e.printStackTrace(); } String r = f.get(); System.out.println(r);} 理解CompletionStage上面提到了CompletableFuture实现的另外一个接口是CompletionStage,从字面上看这是一个完成阶段，在这个接口中声明了许多的方法 虽然这个接口的方法很多，但是很有规律，总的来说，分为对异步结果分别进行响应()，结果转换(Function, BiFunction)，结果消费(Consumer, BiConsumer)，以及异常的处理等。 对异步结果进行转换1public &lt;U&gt; CompletionStage&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn); 例子: 12345678@Testpublic void test() { String result = CompletableFuture.supplyAsync(()-&gt; \"Hello\").thenApplyAsync(s-&gt;{ System.out.println(Thread.currentThread().getName()); return s+\"world\"; }).join(); System.out.println(result);} 对异步结果进行消费1public CompletionStage&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action); 例子： 1234@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; \"Hello\").thenAccept(System.out::println);} 对异步结果不关心，执行下一个操作1public CompletionStage&lt;Void&gt; thenRun(Runnable action); 例子: 1234@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; \"Hello\").thenRun(()-&gt; System.out.println(\"World\"));} 将CompletionStage函数作为参数12public &lt;U&gt; CompletionStage&lt;U&gt; thenCompose (Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn); 例子: 12345@Testpublic void test() { String combine = CompletableFuture.supplyAsync(()-&gt; \"Hello\").thenComposeAsync((x)-&gt; CompletableFuture.supplyAsync(()-&gt; x + \"World\")).join(); System.out.println(combine);} 组合其它的CompletionStage，使用BiFunction计算转换123public &lt;U,V&gt; CompletionStage&lt;V&gt; thenCombine (CompletionStage&lt;? extends U&gt; other, BiFunction&lt;? super T,? super U,? extends V&gt; fn); 例子: 12345@Testpublic void test() { String combine = CompletableFuture.supplyAsync(()-&gt; \"Hello\").thenCombine(CompletableFuture.supplyAsync(()-&gt; \"World\"), (x, y)-&gt; x+y).join(); System.out.println(combine);} 组合其它的CompletionStage，使用BiConsumer消费123public &lt;U&gt; CompletionStage&lt;Void&gt; thenAcceptBoth (CompletionStage&lt;? extends U&gt; other, BiConsumer&lt;? super T, ? super U&gt; action); 例子: 123456@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; \"Hello\").thenAcceptBoth(CompletableFuture.supplyAsync(()-&gt; \"World\"), (x, y)-&gt; { System.out.println(x+y); });} 组合其它的CompletionStage，都完成后执行新的任务12public CompletionStage&lt;Void&gt; runAfterBoth(CompletionStage&lt;?&gt; other, Runnable action); 例子: 123456@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; \"Hello\").runAfterBoth(CompletableFuture.supplyAsync(()-&gt; \"World\"), ()-&gt; { System.out.println(\"ok\"); });} 组合其它的CompletionStage，用计算快的CompletionStage结果进行后续计算转换123public &lt;U&gt; CompletionStage&lt;U&gt; applyToEither (CompletionStage&lt;? extends T&gt; other, Function&lt;? super T, U&gt; fn); 例子: 12345@Testpublic void test() { String e = CompletableFuture.supplyAsync(()-&gt; \"Hello\").applyToEither(CompletableFuture.supplyAsync(()-&gt; \"World\"), (x)-&gt; x).join(); System.out.println(e);} 组合其它的CompletionStage，用计算快的CompletionStage结果进行消费123public CompletionStage&lt;Void&gt; acceptEither (CompletionStage&lt;? extends T&gt; other, Consumer&lt;? super T&gt; action); 例子: 1234@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; \"Hello\").acceptEither(CompletableFuture.supplyAsync(()-&gt; \"World\"), System.out::println);} 组合其它的CompletionStage，其中一个完成后，继续后续任务12public CompletionStage&lt;Void&gt; runAfterEither(CompletionStage&lt;?&gt; other, Runnable action); 例子: 1234@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; \"Hello\").runAfterEither(CompletableFuture.supplyAsync(()-&gt; \"World\"), ()-&gt; System.out.println(\"Hi\"));} 函数完成异常, 使用exceptionally进行熔断12public CompletionStage&lt;T&gt; exceptionally (Function&lt;Throwable, ? extends T&gt; fn); 例子: 12345678910111213@Testpublic void test() { String r = CompletableFuture.supplyAsync(()-&gt; { if (true) { throw new RuntimeException(\"Error Test\"); } return \"Hello\"; }).exceptionally(e -&gt; { e.printStackTrace(); return \"World\"; }).join(); System.out.println(r);} 当运行完成时，对正常和异常的结果进行消费12public CompletionStage&lt;T&gt; whenComplete (BiConsumer&lt;? super T, ? super Throwable&gt; action); 例子: 123456789101112@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; { if (true) { throw new RuntimeException(\"Error Test\"); } return \"Hello\"; }).whenComplete((x, e)-&gt; { System.out.println(x); e.printStackTrace(); });} 对正常和异常的运行结果进行转换12public &lt;U&gt; CompletionStage&lt;U&gt; handle (BiFunction&lt;? super T, Throwable, ? extends U&gt; fn); 例子: 12345678910111213@Testpublic void test() { CompletableFuture.supplyAsync(()-&gt; { if (true) { throw new RuntimeException(\"Error Test\"); } return \"Hello\"; }).handle((x, e)-&gt; { System.out.println(x); e.printStackTrace(); return \"\"; });} 理解CompletableFuture在看完Future和CompletionStage接口之后，我们继续看CompletableFuture里面定义的一些公共方法 构造CompletableFuture 空构造方法构造一个非完成状态的CompletableFuture 通过一个给定的对象创建一个完成状态的CompletableFuture 通过接受一个Supplier接口函数创建","link":"/2019/05/20/理解CompletableFuture/"},{"title":"构建基于kubernetes的DevOps平台(一)","text":"在开始学习k8s前，需要部署一套k8s系统，部署k8s主要有2种方式(我知道的，可能更多)，一种是通过二进制部署，另外一种是通过kubeadm进行部署，这里我们使用kubeadm进行部署。 硬件准备这里我想通过kubeadm部署一个多master的k8s集群，这也更加符合生产环境的需要。更多的k8s的基础知识不想多做赘述。 服务器及IP Hostname Role Ip k8s-m1 master1 172.16.26.81 k8s-m2 master2 172.16.26.82 k8s-m3 master3 172.16.26.83 k8s-n1 node1 172.16.26.121 k8s-n2 node2 172.16.26.122 k8s-n3 node3 172.16.26.123 浮动IP(VIP): 172.16.26.80 环境准备系统配置安装的初始操作系统为centos7.5桌面版，里面已经包含了一部分的系统自带软件，如wget等, 系统配置需要在所有节点执行 关闭SELinux、防火墙1234systemctl stop firewalldsystemctl disable firewalldsetenforce 0sed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config 关闭系统的Swap123swapoff -ayes | cp /etc/fstab /etc/fstab_bakcat /etc/fstab_bak |grep -v swap &gt; /etc/fstab 网络配置12345678modprobe br_netfilterecho \"\"\"vm.swappiness = 0net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1\"\"\" &gt; /etc/sysctl.confsysctl -p 内核升级 安装内核依赖包 1[ ! -f /usr/bin/perl ] &amp;&amp; yum install perl -y 升级内核需要使用 elrepo 的yum 源,首先我们导入 elrepo 的 key并安装 elrepo 源 12 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 查看可用的内核 1yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available --showduplicates 最新内核安装 12 yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available --showduplicates | grep -Po '^kernel-ml.x86_64\\s+\\K\\S+(?=.el7)'yum --disablerepo=\"*\" --enablerepo=elrepo-kernel install -y kernel-ml{,-devel} 修改内核默认的启动顺序 1grub2-set-default 0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfg 重启后查看当前内核 12rebootgrubby --default-kernel IPVS安装IPVS 1yum install ipvsadm ipset sysstat conntrack libseccomp -y 设置加载的内核模块 123456789ipvs_modules=\"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack\"for kernel_module in \\${ipvs_modules}; do /sbin/modinfo -F filename \\${kernel_module} &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe \\${kernel_module} fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs k8s参数设置12345678910111213141516171819202122232425262728293031cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1net.ipv4.neigh.default.gc_stale_time = 120net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.default.arp_announce = 2net.ipv4.conf.lo.arp_announce = 2net.ipv4.conf.all.arp_announce = 2net.ipv4.ip_forward = 1net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 1024net.ipv4.tcp_synack_retries = 2net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.netfilter.nf_conntrack_max = 2310720fs.inotify.max_user_watches=89100fs.may_detach_mounts = 1fs.file-max = 52706963fs.nr_open = 52706963net.bridge.bridge-nf-call-arptables = 1vm.swappiness = 0vm.overcommit_memory=1vm.panic_on_oom=0EOFsysctl --system 软件安装Docker安装123456789101112# 安装docker, 我用的是18.06.3-ceyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repoyum makecache fast## 列出Docker版本yum list docker-ce --showduplicates | sort -r## 安装指定版本sudo yum install docker-ce-&lt;VERSION_STRING&gt; Kubernetes相关组件安装1234567891011cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet kubeadm kubectl ebtables Keepalived Haproxy安装(node节点不需要安装) 安装1yum install -y socat keepalived haproxy 配置变量设置12345678910111213cd ~/# 创建集群信息文件echo &quot;&quot;&quot;CP0_IP=172.16.26.81CP1_IP=172.16.26.82CP2_IP=172.16.26.83VIP=172.16.26.80NET_IF=eth0CIDR=10.244.0.0/16&quot;&quot;&quot; &gt; ./cluster-info# 配置haproxy和keepeelivedbash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/hnbcao/kubeadm-ha-master/v1.14.0/keepalived-haproxy.sh)&quot; 服务开机启动12345678910# 启动dockersed -i \"13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT\" /usr/lib/systemd/system/docker.servicesystemctl daemon-reloadsystemctl enable dockersystemctl start docker# 设置kubelet开机启动systemctl enable kubeletsystemctl enable keepalivedsystemctl enable haproxy 开始部署master节点初始化在开始前配置master1到其它节点的免密登陆，简单-略过 初始化master1 12345678910111213141516171819202122232425echo \"\"\"apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.0controlPlaneEndpoint: \"${VIP}:8443\"maxPods: 100networkPlugin: cniimageRepository: registry.aliyuncs.com/google_containersapiServer: certSANs: - ${CP0_IP} - ${CP1_IP} - ${CP2_IP} - ${VIP}networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: ${CIDR}---#apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs\"\"\" &gt; /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yamlmkdir -p $HOME/.kubecp -f /etc/kubernetes/admin.conf ${HOME}/.kube/config 拷贝相关证书到master2、master3 12345678910111213141516for index in 1 2; do ip=${IPS[${index}]} ssh $ip \"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/\" scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $ip:~/.kube/config ssh ${ip} \"${JOIN_CMD} --experimental-control-plane\"done master2、master3加入节点 12 JOIN_CMD=`kubeadm token create --print-join-command`ssh ${ip} \"${JOIN_CMD} --experimental-control-plane\" node节点加入获取节点加入命令 1kubeadm token create --print-join-command 分别在node上执行获得的命令 集群网络安装网络插件选择 网络插件安装 遇到的问题 docker mtu 的配置 配置/etc/docker/daemon.json 1234567891011121314151617181920 { &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;registry-mirrors&quot;: [&quot;https://axejqb6p.mirror.aliyuncs.com&quot;], &quot;mtu&quot;: 1450, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot;, &quot;max-file&quot;: &quot;3&quot; }} ``` 2. vip的转移我当前使用的配置/etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/statsdefaults mode tcp log global option tcplog option dontlognull option redispatch retries 3 timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout check 10s maxconn 3000listen stats mode http bind :10086 stats enable stats uri /admin?stats stats auth admin:admin stats admin if TRUEfrontend k8s_https *:8443 mode tcp maxconn 2000 default_backend https_sribackend https_sri balance roundrobin server master1-api 172.16.26.81:6443 check inter 10000 fall 2 rise 2 weight 1 server master2-api 172.16.26.82:6443 check inter 10000 fall 2 rise 2 weight 1 server master3-api 172.16.26.83:6443 check inter 10000 fall 2 rise 2 weight 112/etc/keepalived/check_haproxy.sh #!/bin/bashVIRTUAL_IP=172.16.26.80errorExit() { echo “** $“ 1&gt;&amp;2 exit 1}if ip addr | grep -q $VIRTUAL_IP ; then curl -s –max-time 2 –insecure https://${VIRTUAL_IP}:8443/ -o /dev/null || errorExit “Error GET https://${VIRTUAL_IP}:8443/“fi 12 /etc/keepalived/keepalived.conf vrrp_script haproxy-check { script “/bin/bash /etc/keepalived/check_haproxy.sh” interval 3 weight -2 fall 10 rise 2}vrrp_instance haproxy-vip { state BACKUP priority 101 interface eth0 virtual_router_id 47 advert_int 3 unicast_peer { 172.16.26.81 172.16.26.82 172.16.26.83 } virtual_ipaddress { 172.16.26.80 } track_script { haproxy-check }} ` 参考 官方文档 社区教程","link":"/2019/05/27/构建基于kubernetes的DevOps平台(一)/"}],"tags":[{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"springcloud","slug":"springcloud","link":"/tags/springcloud/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"helm","slug":"helm","link":"/tags/helm/"},{"name":"devops","slug":"devops","link":"/tags/devops/"},{"name":"springframework","slug":"springframework","link":"/tags/springframework/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"network","slug":"network","link":"/tags/network/"},{"name":"grpc","slug":"grpc","link":"/tags/grpc/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"}],"categories":[{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"Jenkins","slug":"Jenkins","link":"/categories/Jenkins/"},{"name":"SpringCloud","slug":"Spring/SpringCloud","link":"/categories/Spring/SpringCloud/"},{"name":"SpringFramework","slug":"Spring/SpringFramework","link":"/categories/Spring/SpringFramework/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Network","slug":"Network","link":"/categories/Network/"},{"name":"gRPC","slug":"gRPC","link":"/categories/gRPC/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"}]}